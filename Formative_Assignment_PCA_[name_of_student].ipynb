{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanraisa/Principle--Component--Analysis/blob/main/Formative_Assignment_PCA_%5Bname_of_student%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "#TO DO\n",
        "import numpy as np\n",
        "from numpy.linalg import eig\n"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**[TO DO: Insert Answer here]**"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "By ensuring that data is on the same scale, we enhance the performance, reliability, and interpretability of various machine learning algorithms and data analysis techniques."
      ],
      "metadata": {
        "id": "mkaNmnjSFbBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#TO DO:  Insert code here\n",
        "mean = np.mean(data, axis=0)\n",
        "standardized_data = (data - mean) / np.std(data, axis=0)\n",
        "\n",
        "# the standardized data\n",
        "print(standardized_data)"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cdd4d65-269e-48d4-ffe1-adf80044f80b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "98495b49-9b1d-4694-91ea-ce8e82d72838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2.16666667  -1.06666667  -1.76666667   5.5         -4.36666667]\n",
            " [ -1.06666667   4.26666667   0.46666667  -6.6         19.66666667]\n",
            " [ -1.76666667   0.46666667   1.76666667  -3.3          2.36666667]\n",
            " [  5.5         -6.6         -3.3         24.7        -27.9       ]\n",
            " [ -4.36666667  19.66666667   2.36666667 -27.9         92.56666667]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#TO DO: insert code here\n",
        "cov_matrix = np.cov(data.T)\n",
        "\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " #TO DO: Insert code here for eigenvalues, eigenvectors\n",
        " eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"eigenvalues: \", eigenvalues)\n",
        "print(\"eigenvectors: \", eigenvectors)"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a23797-cba2-464a-daa9-e05ffb7e5a69"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvalues:  [1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "eigenvectors:  [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "7043ab64-f4ac-4098-db06-b97f65a33d95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 2 3 4]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "[Insert Answer here]\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "[insert answer here]\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.Why do we order eigen values and eigen vectors?\n",
        "\n",
        "**Answer:** By ranking our eigenvectors in order of their eigenvalues, highest to lowest, we get the principal components in order of significance.\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "**Answer:** Yes,This will allow us to choose the principal components that explain the most variance in the data."
      ],
      "metadata": {
        "id": "Iwm2nakqDzZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "sum_of_eigenvalues = np.sum(sorted_eigenvalues)\n",
        "\n",
        "print(sum_of_eigenvalues)\n",
        "\n",
        "percentages = (sorted_eigenvalues / sum_of_eigenvalues) * 100\n",
        "\n",
        "print(percentages)\n",
        "\n",
        "# print out the percentages\n",
        "for index, value in enumerate(sorted_eigenvalues):\n",
        "    print(f\"Eigenvalue {index+1}: {value:.8f} ({percentages[index]:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCkWimYE9cX9",
        "outputId": "acd42aae-2aaf-4714-ab61-fc7ce9e68169"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125.46666666666664\n",
            "[8.54607471e+01 1.28977515e+01 1.53964188e+00 1.01684172e-01\n",
            " 1.75348375e-04]\n",
            "Eigenvalue 1: 107.22475075 (85.46%)\n",
            "Eigenvalue 2: 16.18237882 (12.90%)\n",
            "Eigenvalue 3: 1.93173735 (1.54%)\n",
            "Eigenvalue 4: 0.12757974 (0.10%)\n",
            "Eigenvalue 5: 0.00022000 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "#TO DO: Insert code here\n",
        "explained_variance = (sorted_eigenvalues / sum_of_eigenvalues) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "3bb893ce-ff90-4ebb-eef0-51bfa60e6dc1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['85.46%', '12.90%', '1.54%', '0.10%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2  # select the number of principal components\n",
        "\n",
        "reduced_data = sorted_eigenvectors[:, order_of_importance[:k]]\n",
        "reduced_data = np.matmul(standardized_data, reduced_data) # transform the original data"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "cefd93dc-4b91-44ab-c77c-e577e6baf968"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.31389845  1.22362226]\n",
            " [-2.55511419  0.01760889]\n",
            " [ 0.61494463  1.08892909]\n",
            " [-0.03531847 -1.11250845]\n",
            " [ 1.45756867  0.44379893]\n",
            " [-0.7959791  -1.66145072]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "2d5d9dea-4d8c-42e6-a901-130a1dd6e189"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "[insert answer here]"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits(Positive effects):**\n",
        "* PCA helps reduce the number of features in a dataset while retaining most of the original variability. This simplification can make data visualization and interpretation easier and more insightful.\n",
        "\n",
        "* PCA effectively filters out noise and less important details in the data. This can enhance the performance of machine learning models by providing cleaner data inputs.\n",
        "\n",
        "**Limitations(Negative effects):**\n",
        "* The principal components generated by PCA are linear combinations of the original variables, which can be difficult to interpret. This abstraction can obscure the understanding of how original features relate to the outcomes of interest.\n",
        "* Because some data is lost during the PCA process, its safe to say that this can lead to oversimplification or distortion of the data, and make it harder to identify outliers or anomalies."
      ],
      "metadata": {
        "id": "aO6EFb58A0Hl"
      }
    }
  ]
}